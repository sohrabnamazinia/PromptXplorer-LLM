{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c9644ec-3357-4e0f-a81a-1a87f033e631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 prompts, with 5697 secondary rows.\n",
      "                                              prompt  \\\n",
      "0  Portrait of Elon Musk by Gottfried Helnwein an...   \n",
      "1                       kanye west as ezio auditore    \n",
      "2  sonic the hedgehog, in the style of sega genes...   \n",
      "3  a photo of sunflower monster with real human m...   \n",
      "4  dance first. think later. it's the natural ord...   \n",
      "\n",
      "                                             primary  \\\n",
      "0  Portrait of Elon Musk by Gottfried Helnwein an...   \n",
      "1                        kanye west as ezio auditore   \n",
      "2                                 sonic the hedgehog   \n",
      "3  a photo of sunflower monster with real human m...   \n",
      "4  dance first. think later. it's the natural ord...   \n",
      "\n",
      "                                      secondary_list  \n",
      "0                                                 []  \n",
      "1                                                 []  \n",
      "2  [in the style of sega genesis, cartoon, illust...  \n",
      "3                                         [1 0 0 mm]  \n",
      "4  [street dance photography, contemporary dance ...  \n",
      "                                              prompt  \\\n",
      "2  sonic the hedgehog, in the style of sega genes...   \n",
      "2  sonic the hedgehog, in the style of sega genes...   \n",
      "2  sonic the hedgehog, in the style of sega genes...   \n",
      "3  a photo of sunflower monster with real human m...   \n",
      "4  dance first. think later. it's the natural ord...   \n",
      "\n",
      "                                             primary  \\\n",
      "2                                 sonic the hedgehog   \n",
      "2                                 sonic the hedgehog   \n",
      "2                                 sonic the hedgehog   \n",
      "3  a photo of sunflower monster with real human m...   \n",
      "4  dance first. think later. it's the natural ord...   \n",
      "\n",
      "                            secondary  \n",
      "2        in the style of sega genesis  \n",
      "2                             cartoon  \n",
      "2  illustration by jean giraud!!!!!!!  \n",
      "3                            1 0 0 mm  \n",
      "4            street dance photography  \n"
     ]
    }
   ],
   "source": [
    "# Box 1 (final): read each full line, then split with Python’s split()\n",
    "import pandas as pd\n",
    "\n",
    "CSV_PATH = \"prompts.csv\"\n",
    "\n",
    "# 1) Read raw lines\n",
    "with open(CSV_PATH, 'r', encoding='utf-8', newline='') as f:\n",
    "    lines = [line.rstrip('\\r\\n') for line in f if line.strip()]\n",
    "\n",
    "# 2) Drop header if it's literally \"prompt\"\n",
    "if lines and lines[0].strip().lower() == 'prompt':\n",
    "    lines = lines[1:]\n",
    "\n",
    "# 3) Build df exactly as before\n",
    "df = pd.DataFrame({'prompt': lines})\n",
    "\n",
    "# 4) Split on first comma using Python split()\n",
    "def split_prompt(s):\n",
    "    parts = s.split(',', 1)\n",
    "    primary = parts[0].strip()\n",
    "    if len(parts) > 1:\n",
    "        secondaries = [p.strip() for p in parts[1].split(',')]\n",
    "    else:\n",
    "        secondaries = []\n",
    "    return pd.Series([primary, secondaries], index=['primary','secondary_list'])\n",
    "\n",
    "df[['primary','secondary_list']] = df['prompt'].apply(split_prompt)\n",
    "\n",
    "# 5) Explode into df_secondary\n",
    "df_secondary = (\n",
    "    df\n",
    "    .explode('secondary_list')\n",
    "    .rename(columns={'secondary_list': 'secondary'})\n",
    "    .dropna(subset=['secondary'])\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(df)} prompts, with {len(df_secondary)} secondary rows.\")\n",
    "print(df.head())\n",
    "print(df_secondary.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e012b091-d3cd-4f45-8bfc-2522b13b6c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary cluster descriptions: {0: 'epic / intricate / portrait / photo / forms', 1: 'portrait / painting / art / photo / man', 2: 'russia / energy / illustration / forms / forest', 3: 'walter / giving / white / middle / portrait', 4: 'detailed / highly / painting / portrait / working', 5: 'rutkowski / greg / portrait / film / human', 6: 'beautiful / woman / portrait / young / face', 7: 'fat / french / eating / frank / fox', 8: 'photography / dream / winning / award / focused', 9: 'energy / dmt / dream / hyper / artstation'}\n",
      "Secondary cluster descriptions: {0: 'plasma / floating / flare / flame / finnstark', 1: 'painting / digital / aivazovsky / time / realistic', 2: 'artgerm / art / feng / fenghua / ferdinand', 3: 'detailed / highly / portrait / face / beautiful', 4: 'smooth / realistic / high / beautiful / photorealistic', 5: 'intricate / details / detailed / environment / insanely', 6: 'sharp / details / lightning / edges / flame', 7: 'gold / black / accents / sparkling / blood', 8: 'unreal / engine / rendered / render / rutkowski', 9: 'elegant / clothing / beautiful / intricate / ferdinand', 10: 'oil / painting / renaissance / realistic / luminescent', 11: 'trending / artstation / jakub / concept / art', 12: 'eyes / green / blue / red / face', 13: 'concept / art / matte / painting / character', 14: 'mucha / alphonse / rutkowski / greg / artgerm', 15: 'masterpiece / magnum / cgsociety / feng / fenghua', 16: 'octane / rendered / rendering / high / features', 17: 'digital / art / fantasy / award / winning', 18: 'illustration / digital / detailed / isometric / adobe', 19: 'film / grain / noisy / effect / detailded', 20: 'cinematic / composition / atmosphere / epic / style', 21: 'canvas / oil / cinematic / acrylic / textured', 22: 'artstation / style / flower / floating / flare', 23: 'focus / sharp / backlight / smooth / hopper', 24: 'scifi / dystopian / style / character / portrait', 25: 'rossdraws / global / illumination / hdr / aura', 26: 'dark / fantasy / scary / brown / colour', 27: 'fantasy / epic / high / art / character', 28: 'yellow / wearing / gradient / office / room', 29: 'render / octane / aesthetic / photorealistic / detailed', 30: 'hair / black / long / white / blue', 31: 'studio / quality / high / ghibli / lighting', 32: 'volumetric / lighting / outdoor / dim / lights', 33: 'art / nouveau / style / poster / splash', 34: 'hq / artstation / trending / mystical / lighting', 35: 'hyper / realistic / realism / detailed / extremely', 36: 'hd / ultra / textures / wallpaper / quality', 37: 'james / jean / gurney / dark / style', 38: 'sky / clouds / night / moon / stars', 39: 'lighting / cinematic / dynamic / ambient / soft', 40: 'wlop / face / art / feng / fenghua', 41: 'light / volumetric / cinematic / radiant / studio', 42: 'dragon / hair / flower / floating / flare', 43: '8k / resolution / definition / high / uhd', 44: 'greg / rutkowski / art / artgerm / magali', 45: 'centered / artwork / portrait / feng / fenghua', 46: 'rim / light / natural / beautiful / soft', 47: 'dramatic / lighting / cinematic / atmosphere / dynamic', 48: 'tattoos / flower / floating / flare / flame', 49: 'beeple / art / flowers / flower / floating'}\n"
     ]
    }
   ],
   "source": [
    "# Task 2: Vectorize & cluster\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "def cluster_and_describe(texts, n_clusters=10, top_n_terms=5):\n",
    "    vec = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "    X = vec.fit_transform(texts)\n",
    "    km = KMeans(n_clusters=n_clusters, random_state=42).fit(X)\n",
    "    centers = km.cluster_centers_\n",
    "    terms = np.array(vec.get_feature_names_out())\n",
    "    descriptions = {}\n",
    "    for i, center in enumerate(centers):\n",
    "        top_terms = terms[center.argsort()[-top_n_terms:][::-1]]\n",
    "        descriptions[i] = \" / \".join(top_terms)\n",
    "    return km.labels_, descriptions\n",
    "\n",
    "# cluster primaries\n",
    "primaries = df['primary'].tolist()\n",
    "primary_labels, primary_desc = cluster_and_describe(primaries, n_clusters=10)\n",
    "df['primary_cluster'] = primary_labels\n",
    "\n",
    "# cluster secondaries\n",
    "secondaries = df_secondary['secondary'].tolist()\n",
    "sec_labels, sec_desc = cluster_and_describe(secondaries, n_clusters=50)\n",
    "df_secondary['secondary_cluster'] = sec_labels\n",
    "\n",
    "print(\"Primary cluster descriptions:\", primary_desc)\n",
    "print(\"Secondary cluster descriptions:\", sec_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1d10c44-6308-422e-994b-7a60ddb67f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     support   itemsets\n",
      "0      0.897       (P1)\n",
      "1      0.025       (P4)\n",
      "2      0.053       (P6)\n",
      "3      0.082       (S1)\n",
      "4      0.025      (S10)\n",
      "..       ...        ...\n",
      "267    0.017  (S5, S44)\n",
      "268    0.010  (S9, S44)\n",
      "269    0.010  (S5, S47)\n",
      "270    0.021   (S5, S8)\n",
      "271    0.036   (S5, S9)\n",
      "\n",
      "[272 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Task 3 (fixed): Frequent‐itemset mining (size ≤2) on clusters\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import pandas as pd\n",
    "\n",
    "# build cluster_transactions without grouping KeyError\n",
    "cluster_transactions = []\n",
    "for idx, row in df.iterrows():\n",
    "    # one primary per prompt\n",
    "    prim = f\"P{row['primary_cluster']}\"\n",
    "    # lookup secondary clusters for this prompt index (may be none)\n",
    "    sec_clusters = df_secondary.loc[df_secondary.index == idx, 'secondary_cluster'].unique()\n",
    "    secs = [f\"S{c}\" for c in sec_clusters]\n",
    "    cluster_transactions.append([prim] + secs)\n",
    "\n",
    "# now encode and mine\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(cluster_transactions).transform(cluster_transactions)\n",
    "cluster_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "freq_itemsets_2 = apriori(cluster_df, min_support=0.01, use_colnames=True, max_len=2)\n",
    "print(freq_itemsets_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b9e2d06-2361-4703-a857-7bf386b7353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New prompt classified into P1\n",
      "Satellite cluster chain: [np.str_('S4'), np.str_('S23'), np.str_('S13'), np.str_('S22')]\n"
     ]
    }
   ],
   "source": [
    "# Task 4(a): Rebuild vec_primary, km_primary, construct G, define random_walk, and run on a new prompt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import networkx as nx\n",
    "\n",
    "# Re-create the primary vectorizer & k-means from Task 2\n",
    "vec_primary = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_primary   = vec_primary.fit_transform(df['primary'].tolist())\n",
    "km_primary  = KMeans(n_clusters=len(primary_desc), random_state=42).fit(X_primary)\n",
    "\n",
    "# Build directed graph G from association rules on freq_itemsets_2\n",
    "rules = association_rules(freq_itemsets_2, metric=\"confidence\", min_threshold=0.1)\n",
    "G = nx.DiGraph()\n",
    "for _, row in rules.iterrows():\n",
    "    if len(row['antecedents']) == 1 and len(row['consequents']) == 1:\n",
    "        a = next(iter(row['antecedents']))\n",
    "        b = next(iter(row['consequents']))\n",
    "        G.add_edge(a, b, weight=row['confidence'])\n",
    "\n",
    "# Define the random_walk function (only visits new satellite clusters)\n",
    "def random_walk(graph, start, k=5):\n",
    "    path = []\n",
    "    current = start\n",
    "    visited = {start}\n",
    "    for _ in range(k):\n",
    "        neigh = [\n",
    "            (n, d['weight'])\n",
    "            for n, d in graph[current].items()\n",
    "            if n.startswith('S') and n not in visited\n",
    "        ]\n",
    "        if not neigh:\n",
    "            break\n",
    "        nodes, weights = zip(*neigh)\n",
    "        probs = [w / sum(weights) for w in weights]\n",
    "        next_node = np.random.choice(nodes, p=probs)\n",
    "        path.append(next_node)\n",
    "        visited.add(next_node)\n",
    "        current = next_node\n",
    "    return path\n",
    "\n",
    "# Classify a new incoming primary prompt and run the walk\n",
    "new_prompt = \"Describe the optimal solar panel configuration\"\n",
    "X_new      = vec_primary.transform([new_prompt])\n",
    "label      = km_primary.predict(X_new)[0]    # e.g. 2\n",
    "start      = f\"P{label}\"                     # \"P2\"\n",
    "chain      = random_walk(G, start, k=4)\n",
    "\n",
    "print(f\"New prompt classified into P{label}\")\n",
    "print(\"Satellite cluster chain:\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "693031cf-ff9d-4323-a4cb-f8228c2e181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New prompt classified into P1\n",
      "Top 2‑itemsets including this cluster: [(('P1', 'S4', 'S40', 'S41', 'S42'), np.float64(0.6389999999999998))]\n"
     ]
    }
   ],
   "source": [
    "# Task 4(b): Build support matrix & IPF for size‑2 itemsets, handling a new incoming primary prompt\n",
    "\n",
    "import numpy as np\n",
    "# silence divide‑by‑zero / invalid warnings during IPF iterations\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "from ipfn import ipfn   # correct import for the ipfn package\n",
    "\n",
    "# build 2-item support matrix\n",
    "items = list(te.columns_)\n",
    "n = len(items)\n",
    "support = np.zeros((n, n))\n",
    "for i, xi in enumerate(items):\n",
    "    for j, xj in enumerate(items):\n",
    "        if i < j:\n",
    "            mask = cluster_df[xi] & cluster_df[xj]\n",
    "            support[i, j] = mask.mean()\n",
    "            support[j, i] = support[i, j]\n",
    "\n",
    "# prepare marginals and dimensions\n",
    "marginals = [\n",
    "    support.sum(axis=1),  # row sums (for axis‑0 marginals)\n",
    "    support.sum(axis=0)   # col sums (for axis‑1 marginals)\n",
    "]\n",
    "dimensions = [[1], [0]]    # match row marginals to axis 1, col to axis 0\n",
    "\n",
    "# run IPF\n",
    "IPF = ipfn.ipfn(support, marginals, dimensions, convergence_rate=1e-6)\n",
    "est = IPF.iteration()\n",
    "\n",
    "# get top size‑2 itemsets containing a given primary cluster\n",
    "def top_kitemsets(primary, k=2, top_n=5):\n",
    "    idx = items.index(primary)\n",
    "    from itertools import combinations\n",
    "    scores = []\n",
    "    for combo in combinations(range(n), k):\n",
    "        if idx in combo:\n",
    "            prob = est[combo[0], combo[1]]\n",
    "            scores.append((tuple(items[i] for i in combo), prob))\n",
    "    return sorted(scores, key=lambda x: -x[1])[:top_n]\n",
    "\n",
    "# — classify a new incoming primary prompt (just like in 4a) —\n",
    "new_prompt = \"Describe the optimal solar panel configuration\"\n",
    "X_new      = vec_primary.transform([new_prompt])\n",
    "label      = km_primary.predict(X_new)[0]\n",
    "primary    = f\"P{label}\"\n",
    "\n",
    "# — find the top 2‑itemsets that include this primary cluster —\n",
    "top_itemsets = top_kitemsets(primary, k=5, top_n=1)\n",
    "print(f\"New prompt classified into {primary}\")\n",
    "print(\"Top 2‑itemsets including this cluster:\", top_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ede16-bdd7-4166-be64-449ef1f2f624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
